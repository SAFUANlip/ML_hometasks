{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "99e4bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit, KFold, GroupKFold, StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "88b0f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking:\n",
    "    def __init__(self, data, target):\n",
    "        self.df = data.copy()\n",
    "        self.df_ans_train = pd.DataFrame()\n",
    "        self.df_ans_test = pd.DataFrame()\n",
    "        self.df_best_cls = {}\n",
    "        self.scores = {}\n",
    "        self.n_folds = 5\n",
    "        self.scores_propor = 0\n",
    "        \n",
    "        sklearn_nb = naive_bayes.GaussianNB()\n",
    "        sklearn_knn = KNeighborsClassifier(50)\n",
    "        sklearn_lс = LogisticRegression()\n",
    "        sklearn_tree = DecisionTreeClassifier()\n",
    "        sklearn_svc = SVC(kernel='poly', C=1, max_iter=100, probability=True)\n",
    "\n",
    "        self.clf_all = [sklearn_nb, sklearn_knn, sklearn_lс, sklearn_tree, sklearn_svc]\n",
    "        \n",
    "        target = 'G3'\n",
    "        \n",
    "        cols = self.df.columns\n",
    "        labelencoder = LabelEncoder()\n",
    "\n",
    "        num_cols = self.df._get_numeric_data().columns\n",
    "        for i in list(set(cols) - set(num_cols)):\n",
    "            if i != 'sex' and i != 'age' and i != 'address':\n",
    "                self.df[i] = labelencoder.fit_transform(self.df[i].values)\n",
    "        \n",
    "        self.y = self.df[target]\n",
    "        self.X = self.df.drop(target, axis = 1)\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, shuffle=True, random_state=21)\n",
    "        \n",
    "        self.x_train = self.x_train.reset_index()\n",
    "        self.x_train = self.x_train.drop(['index'], axis=1)\n",
    "        self.y_train = self.y_train.reset_index()\n",
    "        self.y_train = self.y_train.drop(['index'], axis=1)\n",
    "        \n",
    "        self.x_test = self.x_test.reset_index()\n",
    "        self.x_test = self.x_test.drop(['index'], axis=1)\n",
    "        self.y_test = self.y_test.reset_index()\n",
    "        self.y_test = self.y_test.drop(['index'], axis=1)\n",
    "        \n",
    "        self.x_test = self.repair(self.x_test)\n",
    "        \n",
    "    def repair(self, x):  \n",
    "        df_wrong, df_right = self.divide_df(df = x)\n",
    "        #print(df_wrong.shape, df_right.shape)\n",
    "        self.repair_df(KNeighborsClassifier(), df_wrong, df_right, ['sex', 'age', 'address'])\n",
    "        x = pd.concat([df_wrong, df_right]).sort_index()\n",
    "        return x\n",
    "        \n",
    "    def divide_df(self, df):\n",
    "        dct = {'M': 1, 'F': 0, 'D': 2, 'C': 3, 'B': 4, 'A': 5}\n",
    "        df['sex'] = df['sex'].map(dct)\n",
    "\n",
    "        df_wrong = df[(df['age'] > 22) | (df['age'] < 15) | ((df['sex'] != 1) & (df['sex'] != 0)) | (df.isnull().any(1))]\n",
    "        df_wrong['address'] = df_wrong['address'].astype('category').cat.codes\n",
    "\n",
    "        df_right = df[~((df['age']>22) | (df['age'] < 15) | ((df['sex'] != 1) & (df['sex'] != 0)) | (df.isnull().any(1)))]\n",
    "        df_right['address'] = df_right['address'].astype('category').cat.codes\n",
    "        return df_wrong, df_right\n",
    "\n",
    "    def divide_df_wrong(self, df_wrong, feat):\n",
    "        if feat == 'sex':\n",
    "            df_wrong_c = df_wrong[(df_wrong['sex'] != 0) & (df_wrong['sex'] != 1)]\n",
    "        elif feat == 'age':\n",
    "            df_wrong_c = df_wrong[(df_wrong['age'] > 22) | (df_wrong['age'] < 15) | (df_wrong['age'].isna())]\n",
    "        elif feat == 'address':\n",
    "            df_wrong_c = df_wrong[(df_wrong['address'] != 0) & (df_wrong['address'] != 1)]\n",
    "        return df_wrong_c\n",
    "\n",
    "    def repair_df(self, algorythm, df_wrong, df_right, feats):\n",
    "        for feat in feats:\n",
    "            X = df_right.drop([feat],axis = 1)\n",
    "            y = df_right[feat]\n",
    "            algorythm.fit(X, y)\n",
    "            X_pred = self.divide_df_wrong(df_wrong, feat)\n",
    "            if X_pred.shape[0] != 0:\n",
    "                X_pred[feat] = algorythm.predict(X_pred[X.columns])\n",
    "                df_wrong[feat].update(X_pred[feat])\n",
    "\n",
    "    def stacking_alg(self):\n",
    "        for clf in self.clf_all:\n",
    "            clf_name = str(clf)[:-2]\n",
    "            meta_feat, best_cls = self.cross_val_predict_st(clf)\n",
    "\n",
    "            self.df_ans_train[clf_name] = meta_feat\n",
    "\n",
    "            \n",
    "            best_cls.fit(self.repair(self.x_train.copy()), self.y_train)\n",
    "            self.df_best_cls[clf_name] = best_cls\n",
    "\n",
    "            self.df_ans_test[clf_name] = self.df_best_cls[clf_name].predict(self.x_test)\n",
    "            self.scores[clf_name] = accuracy_score(self.df_ans_test[clf_name], self.y_test)\n",
    "            print(f'{str(clf)} score = {self.scores[clf_name]}','\\n')\n",
    "            \n",
    "        #lr = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "        #lr.fit(self.df_ans_train, self.y_train)\n",
    "        #print(f'overall score = {accuracy_score(lr.predict(self.df_ans_test), self.y_test)}')\n",
    "        \n",
    "        model = nn.Sequential()\n",
    "        model.add_module('l1', nn.Linear(self.df_ans_train.shape[1], len(self.y.unique())))\n",
    "        # note: layer names must be unique\n",
    "        model.add_module('l2', nn.Softmax())\n",
    "\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        history = []\n",
    "        for i in range(10000):\n",
    "            ix = (self.df_ans_train.sample(frac = 0.85).index)\n",
    "            x_batch = torch.tensor(self.df_ans_train.loc[ix].values, dtype=torch.float32)\n",
    "            y_batch = torch.tensor(self.y_train.squeeze().loc[ix].values, dtype=torch.float32)\n",
    "\n",
    "            y_predicted = model(x_batch)[:, 0]\n",
    "            \n",
    "            loss = F.cross_entropy(y_predicted, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            history.append(loss.data.numpy())\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(\"step #%i | mean loss = %.3f\" % (i, np.mean(history[-10:])))\n",
    "        stacking_pred_proba = model(torch.tensor(self.df_ans_test.values, dtype=torch.float32))\n",
    "        stacking_pred = np.array(stacking_pred_proba.detach().numpy(), dtype=np.int)\n",
    "        print(stacking_pred_proba, stacking_pred_proba.shape, stacking_pred_proba.max())\n",
    "        print(f'overall score = {accuracy_score(stacking_pred, self.y_test)}')\n",
    "        \n",
    "    def cross_val_predict_st(self, clf):\n",
    "        kf = KFold(n_splits = self.n_folds)\n",
    "        meta_feat = np.array([], dtype = 'int64')\n",
    "        \n",
    "        best_score = 1000000\n",
    "        best_cls = clf\n",
    "        \n",
    "        for train_index, feach_index in kf.split(self.x_train):\n",
    "            x_train, x_feach = self.x_train.loc[train_index], self.x_train.loc[feach_index]\n",
    "            y_train, y_feach = self.y_train.loc[train_index], self.y_train.loc[feach_index]\n",
    "            \n",
    "            x_train = self.repair(x_train)\n",
    "            x_feach = self.repair(x_feach)\n",
    "            \n",
    "            clf.fit(x_train,y_train)\n",
    "            predict = clf.predict(x_feach)\n",
    "            \n",
    "            if accuracy_score(predict, y_feach) < best_score:\n",
    "                best_cls = clf\n",
    "            \n",
    "            meta_feat = np.concatenate((meta_feat, predict))\n",
    "        return meta_feat, best_cls\n",
    "    \n",
    "    def scor_prop(self):\n",
    "        w_sum = sum(self.scores.values())\n",
    "        self.scores_propor = {cls: w / w_sum  for cls, w in self.scores.items()}\n",
    "        \n",
    "    def bar_plot_sc_prop(self):\n",
    "        self.scor_prop()\n",
    "        sns.barplot(y=list(self.scores_propor.keys()), x = list(self.scores_propor.values()), palette=\"rocket\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.title('proportion of algorithms', fontsize=16);\n",
    "        \n",
    "    def correlation_of_clf(self):\n",
    "        f = plt.figure(figsize=(10, 9))\n",
    "        plt.matshow(self.df_ans_train.corr(), fignum=f.number)\n",
    "        plt.xticks(range(self.df_ans_train.select_dtypes(['number']).shape[1]), self.df_ans_train.select_dtypes(['number']).columns, fontsize=14, rotation=90)\n",
    "        plt.yticks(range(self.df_ans_train.select_dtypes(['number']).shape[1]), self.df_ans_train.select_dtypes(['number']).columns, fontsize=14)\n",
    "        cb = plt.colorbar()\n",
    "        cb.ax.tick_params(labelsize=14)\n",
    "        plt.title('Correlation Matrix', fontsize=16);\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "0a1275ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_features_with_answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "9090f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stack = Stacking(data, target = 'G3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "bb92e2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB() score = 0.04395604395604396 \n",
      "\n",
      "KNeighborsClassifier(n_neighbors=50) score = 0.12087912087912088 \n",
      "\n",
      "LogisticRegression() score = 0.12087912087912088 \n",
      "\n",
      "DecisionTreeClassifier() score = 0.06593406593406594 \n",
      "\n",
      "SVC(C=1, kernel='poly', max_iter=100, probability=True) score = 0.17582417582417584 \n",
      "\n",
      "step #0 | mean loss = 20972.564\n",
      "step #1000 | mean loss = 21100.416\n",
      "step #2000 | mean loss = 20984.031\n",
      "step #3000 | mean loss = 21121.057\n",
      "step #4000 | mean loss = 21099.844\n",
      "step #5000 | mean loss = 21058.564\n",
      "step #6000 | mean loss = 21052.830\n",
      "step #7000 | mean loss = 21057.416\n",
      "step #8000 | mean loss = 21043.656\n",
      "step #9000 | mean loss = 21089.525\n",
      "tensor([[1.0410e-21, 6.5797e-04, 8.8835e-03,  ..., 5.1099e-09, 5.2787e-01,\n",
      "         9.8174e-17],\n",
      "        [4.8663e-19, 9.8745e-04, 1.1941e-06,  ..., 3.6207e-07, 5.2630e-03,\n",
      "         1.2987e-14],\n",
      "        [2.2991e-21, 1.6283e-03, 5.7807e-03,  ..., 8.5858e-09, 6.1898e-01,\n",
      "         1.8790e-16],\n",
      "        ...,\n",
      "        [2.1345e-18, 2.6299e-03, 1.2729e-02,  ..., 2.8396e-07, 3.6446e-01,\n",
      "         1.7723e-14],\n",
      "        [7.3270e-20, 4.5333e-04, 3.7503e-03,  ..., 4.6200e-08, 3.7129e-01,\n",
      "         2.3658e-15],\n",
      "        [1.0791e-19, 1.0919e-04, 4.4830e-03,  ..., 6.0090e-08, 2.3078e-01,\n",
      "         2.2651e-15]], grad_fn=<SoftmaxBackward0>) torch.Size([91, 17]) tensor(0.9996, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [428]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstacking_alg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [425]\u001b[0m, in \u001b[0;36mStacking.stacking_alg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m stacking_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(stacking_pred_proba\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(stacking_pred_proba, stacking_pred_proba\u001b[38;5;241m.\u001b[39mshape, stacking_pred_proba\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverall score = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(stacking_pred, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\AnacondaGO\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\AnacondaGO\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass targets"
     ]
    }
   ],
   "source": [
    "data_stack.stacking_alg() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stack.y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af728129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
